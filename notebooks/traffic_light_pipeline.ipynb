{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba0f54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import & setup\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e305c56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: Paths & global config\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/lisa-traffic-light-dataset\"\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "\n",
    "DAY_TRAIN = os.path.join(DATA_ROOT, \"dayTrain\")\n",
    "NIGHT_TRAIN = os.path.join(DATA_ROOT, \"nightTrain\")\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 25\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"DATA_ROOT exists:\", os.path.exists(DATA_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e131f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3: Label mapping\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"stop\": \"RED\",\n",
    "    \"warning\": \"YELLOW\",\n",
    "    \"go\": \"GREEN\",\n",
    "    \"goLeft\": \"GREEN\",\n",
    "    \"goForward\": \"GREEN\"\n",
    "}\n",
    "\n",
    "CLASS_TO_IDX = {\"RED\": 0, \"YELLOW\": 1, \"GREEN\": 2}\n",
    "IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "\n",
    "LABEL_MAP, CLASS_TO_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83918db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: Load input image frame\n",
    "\n",
    "img_path = os.path.join(\n",
    "    DAY_TRAIN,\n",
    "    \"dayClip1\",\n",
    "    \"frames\",\n",
    "    \"dayClip1--00001.jpg\"\n",
    ")\n",
    "\n",
    "frame = cv2.imread(img_path)\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Input camera frame\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a097f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: Load annotation CSV\n",
    "\n",
    "csv_path = os.path.join(\n",
    "    DAY_TRAIN,\n",
    "    \"dayClip1\",\n",
    "    \"frameAnnotationsBOX.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_path, sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbbb89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 6: Get bounding boxes for this frame\n",
    "\n",
    "frame_name = \"dayClip1--00001.jpg\"\n",
    "rows = df[df[\"Filename\"] == frame_name]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6fb3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 7: Crop ROIs\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "rois, labels = [], []\n",
    "\n",
    "for _, r in rows.iterrows():\n",
    "    label = r[\"Annotation tag\"]\n",
    "    if label not in LABEL_MAP:\n",
    "        continue\n",
    "    \n",
    "    crop = img.crop((\n",
    "        int(r[\"Upper left corner X\"]),\n",
    "        int(r[\"Upper left corner Y\"]),\n",
    "        int(r[\"Lower right corner X\"]),\n",
    "        int(r[\"Lower right corner Y\"])\n",
    "    )).resize((IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    rois.append(crop)\n",
    "    labels.append(LABEL_MAP[label])\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for i, roi in enumerate(rois):\n",
    "    plt.subplot(1, len(rois), i+1)\n",
    "    plt.imshow(roi)\n",
    "    plt.title(labels[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e688f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 8: CNN inference (placeholder)\n",
    "\n",
    "predicted_states = labels  # temporary, replace with model prediction\n",
    "predicted_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ddf41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 9: Rule-based planner\n",
    "\n",
    "def stopping_distance(v, a=3.0, t=0.7):\n",
    "    return (v*v)/(2*a) + v*t\n",
    "\n",
    "def planner(light, v, d):\n",
    "    if light == \"RED\":\n",
    "        return \"STOP\"\n",
    "    if light == \"GREEN\":\n",
    "        return \"GO\"\n",
    "    if light == \"YELLOW\":\n",
    "        return \"STOP\" if d > stopping_distance(v) else \"PROCEED\"\n",
    "\n",
    "speed = 10.0      # m/s\n",
    "distance = 25.0   # m\n",
    "\n",
    "for state in predicted_states:\n",
    "    print(state, \"â†’\", planner(state, speed, distance))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
