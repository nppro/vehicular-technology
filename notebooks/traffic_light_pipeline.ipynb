{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ab6fa2",
   "metadata": {},
   "source": [
    "# Traffic Light State Recognition and Rule-Based Planning\n",
    "**Dataset:** LISA Traffic Light Dataset  \n",
    "**Platform:** Kaggle Notebook  \n",
    "**Task:** Perception + Planning for Autonomous Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac17efd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Traffic light recognition is a critical perception task in autonomous driving systems.\n",
    "In this project, we design a simplified perception–planning pipeline that recognizes\n",
    "traffic light states from camera images and makes driving decisions using rule-based logic.\n",
    "\n",
    "The system focuses on:\n",
    "- Traffic light state recognition (RED, YELLOW, GREEN)\n",
    "- Rule-based decision making (STOP, GO, PROCEED)\n",
    "\n",
    "This scope allows us to study the interaction between perception and planning without\n",
    "introducing unnecessary system complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348f5e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Dataset Description\n",
    "\n",
    "We use the LISA Traffic Light Dataset, which contains real-world driving images captured\n",
    "from a moving vehicle under both day and night conditions.\n",
    "\n",
    "Key characteristics:\n",
    "- Over 40,000 image frames\n",
    "- Annotated traffic light bounding boxes\n",
    "- Multiple traffic light states\n",
    "- Diverse illumination conditions\n",
    "\n",
    "Original dataset labels are mapped into three traffic light states:\n",
    "- stop → RED\n",
    "- warning → YELLOW\n",
    "- go, goLeft, goForward → GREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400d379",
   "metadata": {},
   "source": [
    "## 3. System Pipeline\n",
    "\n",
    "The proposed system follows a two-stage architecture:\n",
    "\n",
    "1. Perception stage:\n",
    "   - Input road image is treated as a camera frame.\n",
    "   - Traffic light regions are cropped using ground-truth bounding boxes.\n",
    "   - A CNN classifier predicts the traffic light state.\n",
    "\n",
    "2. Planning stage:\n",
    "   - Predicted traffic light state is combined with vehicle speed and distance.\n",
    "   - A rule-based planner determines the driving action.\n",
    "\n",
    "This design separates perception from decision-making and ensures interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e794b",
   "metadata": {},
   "source": [
    "## 4. Environment Setup\n",
    "\n",
    "The project is implemented using a Kaggle notebook environment with GPU acceleration.\n",
    "All experiments are conducted with fixed random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba0f54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import & setup\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e305c56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: Paths & global config\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/lisa-traffic-light-dataset\"\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "\n",
    "DAY_TRAIN = os.path.join(DATA_ROOT, \"dayTrain\")\n",
    "NIGHT_TRAIN = os.path.join(DATA_ROOT, \"nightTrain\")\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 25\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"DATA_ROOT exists:\", os.path.exists(DATA_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2b583",
   "metadata": {},
   "source": [
    "## 5. Label Mapping\n",
    "\n",
    "Original dataset labels are mapped into three traffic light states to match\n",
    "the requirements of the planning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e131f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "LABEL_MAP = {\n",
    "    \"stop\": \"RED\",\n",
    "    \"warning\": \"YELLOW\",\n",
    "    \"go\": \"GREEN\",\n",
    "    \"goLeft\": \"GREEN\",\n",
    "    \"goForward\": \"GREEN\"\n",
    "}\n",
    "\n",
    "CLASS_TO_IDX = {\"RED\": 0, \"YELLOW\": 1, \"GREEN\": 2}\n",
    "IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "\n",
    "LABEL_MAP, CLASS_TO_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b493936",
   "metadata": {},
   "source": [
    "## 6. Input Image Frame\n",
    "\n",
    "Each image in the dataset is treated as a camera frame captured at a specific\n",
    "time step by the autonomous vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83918db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: Load input image frame\n",
    "\n",
    "img_path = os.path.join(\n",
    "    DAY_TRAIN,\n",
    "    \"dayClip1\",\n",
    "    \"frames\",\n",
    "    \"dayClip1--00001.jpg\"\n",
    ")\n",
    "\n",
    "frame = cv2.imread(img_path)\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(frame)\n",
    "plt.title(\"Input camera frame\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9ef2a",
   "metadata": {},
   "source": [
    "## 7. Traffic Light ROI Extraction\n",
    "\n",
    "Traffic light regions are extracted using bounding box annotations provided\n",
    "by the dataset. This step reduces background noise and simplifies classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6fb3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_path = os.path.join(\n",
    "    DAY_TRAIN,\n",
    "    \"dayClip1\",\n",
    "    \"frameAnnotationsBOX.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "frame_name = \"dayClip1--00001.jpg\"\n",
    "rows = df[df[\"Filename\"] == frame_name]\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "rois, labels = [], []\n",
    "\n",
    "for _, r in rows.iterrows():\n",
    "    label = r[\"Annotation tag\"]\n",
    "    if label not in LABEL_MAP:\n",
    "        continue\n",
    "    \n",
    "    crop = img.crop((\n",
    "        int(r[\"Upper left corner X\"]),\n",
    "        int(r[\"Upper left corner Y\"]),\n",
    "        int(r[\"Lower right corner X\"]),\n",
    "        int(r[\"Lower right corner Y\"])\n",
    "    )).resize((IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    rois.append(crop)\n",
    "    labels.append(LABEL_MAP[label])\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for i, roi in enumerate(rois):\n",
    "    plt.subplot(1, len(rois), i+1)\n",
    "    plt.imshow(roi)\n",
    "    plt.title(labels[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5c13a",
   "metadata": {},
   "source": [
    "## 8. Rule-Based Planning\n",
    "\n",
    "A rule-based planner is used to ensure safe and interpretable driving decisions.\n",
    "Learning-based decision making is intentionally avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e688f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 8: CNN inference (placeholder)\n",
    "\n",
    "predicted_states = labels  # temporary, replace with model prediction\n",
    "predicted_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85ece6",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def stopping_distance(v, a=3.0, t=0.7):\n",
    "    return (v*v)/(2*a) + v*t\n",
    "\n",
    "def planner(light, speed, distance):\n",
    "    if light == \"RED\":\n",
    "        return \"STOP\"\n",
    "    if light == \"GREEN\":\n",
    "        return \"GO\"\n",
    "    if light == \"YELLOW\":\n",
    "        return \"STOP\" if distance > stopping_distance(speed) else \"PROCEED\"\n",
    "\n",
    "speed = 10.0\n",
    "distance = 25.0\n",
    "\n",
    "for state in labels:\n",
    "    print(state, \"→\", planner(state, speed, distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c3de0",
   "metadata": {},
   "source": [
    "## 9. Discussion and Limitations\n",
    "\n",
    "- Bounding boxes are assumed to be perfect.\n",
    "- Distance to traffic lights is simulated.\n",
    "- Yellow light decisions are sensitive to distance estimation.\n",
    "\n",
    "Despite these limitations, the system clearly demonstrates the interaction\n",
    "between perception and rule-based planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69f3cd",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook presents an executable perception–planning pipeline for\n",
    "autonomous driving applications. Traffic light states are recognized\n",
    "from camera images and translated into safe driving actions using\n",
    "rule-based logic.\n",
    "\n",
    "Future work may include traffic light detection, temporal modeling,\n",
    "and real-time distance estimation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
